{"meta":{"title":"Jason's Blog","subtitle":null,"description":"自律给我自由","author":"Jason Sun","url":"http://sunlanchang.github.io"},"pages":[{"title":"关于","date":"2017-10-20T16:47:49.000Z","updated":"2017-10-22T11:33:44.605Z","comments":true,"path":"about/index.html","permalink":"http://sunlanchang.github.io/about/index.html","excerpt":"","text":"尝试使用hexo来写博客，看看有什么惊喜的地方。"},{"title":"categories","date":"2017-10-22T14:44:02.000Z","updated":"2017-10-22T14:44:28.603Z","comments":true,"path":"categories/index.html","permalink":"http://sunlanchang.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-10-22T14:43:24.000Z","updated":"2017-10-22T14:43:53.880Z","comments":true,"path":"tags/index.html","permalink":"http://sunlanchang.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"CRC冗余码","slug":"CRC冗余码","date":"2017-10-26T02:38:07.000Z","updated":"2017-10-26T02:40:39.847Z","comments":true,"path":"2017/10/26/CRC冗余码/","link":"","permalink":"http://sunlanchang.github.io/2017/10/26/CRC冗余码/","excerpt":"","text":"CRC校验基本原理CRC检验原理实际上就是在一个p位二进制数据序列之后附加一个r位二进制检验码(序列)，从而构成一个总长为n＝p＋r位的二进制序列；附加在数据序列之后的这个检验码与数据序列的内容之间存在着某种特定的关系。如果因干扰等原因使数据序列中的某一位或某些位发生错误，这种特定关系就会被破坏。因此，通过检查这一关系，就可以实现对数据正确性的检验。 几个基本概念 帧检验序列FCS（Frame Check Sequence）：为了进行差错检验而添加的冗余码。 多项式模2运行：实际上是按位异或(Exclusive OR)运算，即相同为0，相异为1，也就是不考虑进位、借位的二进制加减运算。如：$10011011 + 11001010 = 01010001$。 生成多项式（generator polynomial）：当进行CRC检验时，发送方与接收方需要事先约定一个除数，即生成多项式，一般记作$G(x)$。生成多项式的最高位与最低位必须是1。常用的CRC码的生成多项式有：$CRC8=X^8+X^5+X4+1$$CRC-CCITT=X^16+X^12+X^5+1$$CRC16=X^16+X^15+X^5+1$$CRC12=X^12+X^11+X^3+X^2+1$$CRC32=X^32+X^26+X^23+X^22+X^16+X^12+X^11+X^10+X^8+X^7+X^5+X^4+X^2+X^1+1$每一个生成多项式都可以与一个代码相对应，如CRC8对应代码：100110001。CRC检验码的计算设信息字段为K位，校验字段为R位，则码字长度为$N(N=K+R)$。设双方事先约定了一个R次多项式$g(x)$，则CRC码：$V(x)=A(x)g(x)=xRm(x)+r(x)$其中:$m(x)$为K次信息多项式， $r(x)$为$R-1$次校验多项式。这里$r(x)$对应的代码即为冗余码，加在原信息字段后即形成CRC码。r(x)的计算方法为：在K位信息字段的后面添加R个0，再除以$g(x)$对应的代码序列，得到的余数即为$r(x)$对应的代码(应为R－1位；若不足，而在高位补0)。 计算示例 这里$g(x)=11001$，生成的CRC冗余码为$1010$，最终得到的码字是$1011001010$即被除数+冗余码。错误检测当接收方收到数据后，用收到的数据对P（事先约定的）进行模2除法，若余数为0，则认为数据传输无差错；若余数不为0，则认为数据传输出现了错误，由于不知道错误发生在什么地方，因而不能进行自动纠正，一般的做法是丢弃接收的数据。 CRC是一种常用的检错码，并不能用于自动纠错。 只要经过严格的挑选，并使用位数足够多的除数 P，那么出现检测不到的差错的概率就很小很小。 仅用循环冗余检验 CRC 差错检测技术只能做到无差错接受（只是非常近似的认为是无差错的），并不能保证可靠传输。","categories":[],"tags":[]},{"title":"利用jieba和wordcloud生成词云","slug":"利用jieba和wordcloud生成词云","date":"2017-10-26T00:20:44.000Z","updated":"2017-10-26T00:27:42.215Z","comments":true,"path":"2017/10/26/利用jieba和wordcloud生成词云/","link":"","permalink":"http://sunlanchang.github.io/2017/10/26/利用jieba和wordcloud生成词云/","excerpt":"","text":"利用wordcloud和jieba做一个词云环境使用到的轮子：matplotlib，jieba，scipy，wordcloud，numpy，PIL。python2对jieba的中文分词支持不是很好，所以使用python3。1234567python 3.6.1jieba 0.39matplotlib 1.0.4scipy 1.0.0wordcloud 1.3.1numpy 1.13.3PIL 1.1.6 具体实现导入轮子1234567#encoding=utf-8import jiebaimport matplotlib.pyplot as pltfrom scipy.misc import imreadfrom wordcloud import WordCloud, STOPWORDS, ImageColorGeneratorimport numpy as npfrom PIL import Image 所用到的库都能用pip3安装。 生成分词123text_from_file=open('data.txt','r').read()Word_spilt_jieba = jieba.cut(text_from_file,cut_all = False)word_space = ' '.join(Word_spilt_jieba) 数据是从网上复制了几份十九大，国庆，小说的内容，从文本中读入数据到text，用jieba进行分词，不使用全模式，全模式匹配会出现重复关键词的现象，使用后效果并不好。 自定义词云背景12img=imread('bipt.jpg')img = np.array(Image.open('bipt.jpg')) 设置生成词云的背景，这里用到了numpy将图片转换为矩阵，图片需要自己下载定义背景。 生成词云12345678910my_wordcloud = WordCloud( background_color='white', #设置背景颜色 mask=img, #背景图片 max_words = 200, #设置最大显示的词数 stopwords = STOPWORDS, #设置停用词 #设置字体格式，字体格式 .ttf文件需自己网上下载，最好将名字改为英文，中文名路径加载会出现问题。 font_path = 'simkai.ttf', max_font_size = 100, #设置字体最大值 random_state=50, #设置随机生成状态，即多少种配色方案 ).generate(word_space) 设置wordcloud参数，注意这里有一个字体必须自己设置中文字体，否则生成的词云不能出现中文，我用到的是simkai.ttf，下载地址：simkai字体。 显示词云12345iamge_colors = ImageColorGenerator(img)plt.imshow(my_wordcloud)plt.axis('off')plt.show()my_wordcloud.to_file('res.jpg') 取图片的颜色作为词云的颜色，并显示词云。如下图","categories":[],"tags":[{"name":"小程序","slug":"小程序","permalink":"http://sunlanchang.github.io/tags/小程序/"}]},{"title":"爬取QQ空间数据进行分析","slug":"爬取QQ空间数据进行分析","date":"2017-10-24T00:45:21.000Z","updated":"2017-10-26T00:40:17.000Z","comments":true,"path":"2017/10/24/爬取QQ空间数据进行分析/","link":"","permalink":"http://sunlanchang.github.io/2017/10/24/爬取QQ空间数据进行分析/","excerpt":"","text":"主要思路： 通过selenium+phantomjs模拟登录qq空间取到cookies和g_qzonetoken，并算出gtk 通过Requests库利用前面得到的url参数，构造http请求 分析请求得到的响应，是一个json，利用正则表达式提取字段 设计数据表，并将提取到的字段插入到数据库中 通过qq邮箱中的导出联系人功能，把好友的qq号导出到一个csv文件，遍历所有的qq号爬取所有的说说 通过sql查询和ipython分析数据，并将数据可视化 通过python的第三方库jieba、wordcloud基于说说的内容做一个词云 通过selenium+phantomjs模拟登录qq空间取到cookies和g_qzonetoken，并算出gtk具体实现1234567891011121314151617181920212223242526272829303132333435import refrom selenium import webdriverfrom time import sleepfrom PIL import Image#定义登录函数def QR_login(): def getGTK(cookie): \"\"\" 根据cookie得到GTK \"\"\" hashes = 5381 for letter in cookie['p_skey']: hashes += (hashes &lt;&lt; 5) + ord(letter)return hashes &amp; 0x7fffffff browser=webdriver.PhantomJS(executable_path=\"D:\\phantomjs.exe\")#这里要输入你的phantomjs所在的路径 url=\"https://qzone.qq.com/\"#QQ登录网址 browser.get(url) browser.maximize_window()#全屏 sleep(3)#等三秒 browser.get_screenshot_as_file('QR.png')#截屏并保存图片 im = Image.open('QR.png')#打开图片 im.show()#用手机扫二维码登录qq空间 sleep(20)#等二十秒，可根据自己的网速和性能修改 print(browser.title)#打印网页标题 cookie = &#123;&#125;#初始化cookie字典 for elem in browser.get_cookies():#取cookies cookie[elem['name']] = elem['value']print('Get the cookie of QQlogin successfully!(共%d个键值对)' % (len(cookie))) html = browser.page_source#保存网页源码 g_qzonetoken=re.search(r'window\\.g_qzonetoken = \\(function\\(\\)\\&#123; try\\&#123;return (.*?);\\&#125; catch\\(e\\)',html)#从网页源码中提取g_qzonetoken gtk=getGTK(cookie)#通过getGTK函数计算gtk browser.quit()return (cookie,gtk,g_qzonetoken.group(1))if __name__==\"__main__\": QR_login() 通过火狐浏览器的一个叫json-dataview的插件可以看到这个响应是一个json格式的12345678910111213141516171819202122232425262728293031323334353637383940414243444546def parse_mood(i): '''从返回的json中，提取我们想要的字段''' text = re.sub('\"commentlist\":.*?\"conlist\":', '', i)if text: myMood = &#123;&#125; myMood[\"isTransfered\"] = False tid = re.findall('\"t1_termtype\":.*?\"tid\":\"(.*?)\"', text)[0] # 获取说说ID tid = qq + '_' + tid myMood['id'] = tid myMood['pos_y'] = 0 myMood['pos_x'] = 0 mood_cont = re.findall('\\],\"content\":\"(.*?)\"', text)if re.findall('&#125;,\"name\":\"(.*?)\",', text): name = re.findall('&#125;,\"name\":\"(.*?)\",', text)[0] myMood['name'] = nameif len(mood_cont) == 2: # 如果长度为2则判断为属于转载 myMood[\"Mood_cont\"] = \"评语:\" + mood_cont[0] + \"---------&gt;转载内容:\" + mood_cont[1] # 说说内容 myMood[\"isTransfered\"] = True elif len(mood_cont) == 1: myMood[\"Mood_cont\"] = mood_cont[0]else: myMood[\"Mood_cont\"] = \"\" if re.findall('\"created_time\":(\\d+)', text): created_time = re.findall('\"created_time\":(\\d+)', text)[0] temp_pubTime = datetime.datetime.fromtimestamp(int(created_time)) temp_pubTime = temp_pubTime.strftime(\"%Y-%m-%d %H:%M:%S\") dt = temp_pubTime.split(' ') time = dt[1] myMood['time'] = time date = dt[0] myMood['date'] = dateif re.findall('\"source_name\":\"(.*?)\"', text): source_name = re.findall('\"source_name\":\"(.*?)\"', text)[0] # 获取发表的工具（如某手机） myMood['tool'] = source_nameif re.findall('\"pos_x\":\"(.*?)\"', text):#获取经纬度坐标 pos_x = re.findall('\"pos_x\":\"(.*?)\"', text)[0] pos_y = re.findall('\"pos_y\":\"(.*?)\"', text)[0]if pos_x: myMood['pos_x'] = pos_xif pos_y: myMood['pos_y'] = pos_y idname = re.findall('\"idname\":\"(.*?)\"', text)[0] myMood['idneme'] = idname cmtnum = re.findall('\"cmtnum\":(.*?),', text)[0] myMood['cmtnum'] = cmtnumreturn myMood#返回一个字典 我们想要的东西已经提取出来了，接下来需要设计数据表，通过navicat可以很方便的建表，然后通过python连接mysql数据库，写入数据。这是创建数据表的sql代码1234567891011121314CREATE TABLE `mood` (`name` varchar(80) DEFAULT NULL,`date` date DEFAULT NULL,`content` text,`comments_num` int(11) DEFAULT NULL,`time` time DEFAULT NULL,`tool` varchar(255) DEFAULT NULL,`id` varchar(255) NOT NULL,`sitename` varchar(255) DEFAULT NULL,`pox_x` varchar(30) DEFAULT NULL,`pox_y` varchar(30) DEFAULT NULL,`isTransfered` double DEFAULT NULL,PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 其实到这里爬虫的主要的代码就算完了，之后主要是通过QQ邮箱的联系人导出功能，构建url列表，最后等着它运行完成就可以了。这里我单线程爬200多个好友用了大约三个小时，拿到了十万条说说。下面是爬虫的主体代码。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#从csv文件中取qq号，并保存在一个列表中csv_reader = csv.reader(open('qq.csv'))friend=[]for row in csv_reader: friend.append(row[3])friend.pop(0)friends=[]for f in friend: f=f[:-7] friends.append(f)headers=&#123;'Host': 'h5.qzone.qq.com', 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:53.0) Gecko/20100101 Firefox/53.0', 'Accept': '*/*', 'Accept-Language':'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3', 'Accept-Encoding': 'gzip, deflate, br', 'Referer': 'https://user.qzone.qq.com/790178228?_t_=0.22746974226377736', 'Connection':'keep-alive'&#125;#伪造浏览器头conn = MySQLdb.connect('localhost', 'root', '123456', 'qq_mood', charset=\"utf8\", use_unicode=True)#连接mysql数据库cursor = conn.cursor()#定义游标cookie,gtk,qzonetoken=QRlogin#通过登录函数取得cookies，gtk，qzonetokens=requests.session()#用requests初始化会话for qq in friends:#遍历qq号列表 for p in range(0,1000): pos=p*20 params=&#123;'uin':qq, 'ftype':'0', 'sort':'0', 'pos':pos, 'num':'20', 'replynum':'100', 'g_tk':gtk, 'callback':'_preloadCallback', 'code_version':'1', 'format':'jsonp', 'need_private_comment':'1', 'qzonetoken':qzonetoken &#125; response=s.request('GET','https://h5.qzone.qq.com/proxy/domain/taotao.qq.com/cgi-bin/emotion_cgi_msglist_v6',params=params,headers=headers,cookies=cookie)print(response.status_code)#通过打印状态码判断是否请求成功 text=response.text#读取响应内容 if not re.search('lbs', text):#通过lbs判断此qq的说说是否爬取完毕 print('%s说说下载完成'% qq)break textlist = re.split('\\&#123;\"certified\"', text)[1:]for i in textlist: myMood=parse_mood(i)'''将提取的字段值插入mysql数据库，通过用异常处理防止个别的小bug中断爬虫，开始的时候可以先不用异常处理判断是否能正常插入数据库''' try: insert_sql = ''' insert into mood(id,content,time,sitename,pox_x,pox_y,tool,comments_num,date,isTransfered,name) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s) ''' cursor.execute(insert_sql, (myMood['id'],myMood[\"Mood_cont\"],myMood['time'],myMood['idneme'],myMood['pos_x'],myMood['pos_y'],myMood['tool'],myMood['cmtnum'],myMood['date'],myMood[\"isTransfered\"],myMood['name'])) conn.commit()except: passprint('说说全部下载完成！') 数据分析学生QQ用的多还是微信用的多？ 先用sql进行聚合分析，然后通过ipython作图，将数据可视化。统计一年之中每天的说说数目，可以发现每年除夕这一天是大家发说说最多的一天，可以看出2015年9月达到了一个高峰，主要因为数据是2015级的，所以在2015年九月大学入学的，之后开始下降，好多人开始玩微信，逐渐放弃了QQ，所以北石化学生用微信还是多。 通过下面这个年变化图可以更直观的看出QQ使用的频率越来越少，可能因为大学里班级，社团，活动有很多的微信群，越来越少的北石化学生使用QQ。 学生晚上几点睡觉？通过这个每小时段说说发表的数目柱形图，可以发现大家在晚上22点到23点左右是最多的，另外中午十二点到一点也有一个小高峰!由此可见大多数学生在宿舍十一点熄灯后，并不会按时睡觉。 学生的经济情况怎么样？用Excel的内容筛选功能，做了一个手机类型的饼图，通过这个饼图可以看出使用最多的手机是苹果，小米，魅族，华为这四个手机品牌，说明大多数大学生还是比较倾向于性价比比较高的手机，从某一方面可以体现大多数同学还是中等生活水平。 学生都在说些什么？通过将mood表中的content字段导出为txt文本文件，利用python的jieba和wordcloud这两个第三方库，可以生成基于说说内容的词云.看看大家在国庆期间都再说些什么，很明显关于，习近平主席，国庆节，人民英雄纪念碑等关键词明显增多，同样也有计算机，英语，考试等关于学习的字段。 ，","categories":[],"tags":[{"name":"小程序","slug":"小程序","permalink":"http://sunlanchang.github.io/tags/小程序/"}]},{"title":"Python写一个简单的神经网络","slug":"测试文档","date":"2017-10-20T16:43:50.000Z","updated":"2017-10-22T11:31:44.440Z","comments":true,"path":"2017/10/21/测试文档/","link":"","permalink":"http://sunlanchang.github.io/2017/10/21/测试文档/","excerpt":"","text":"简单的神经网络算法，包括基本的后向传播BP算法，前向传播算法，更新权重使用的梯度下降算法，基本的框架算是有了，学习使用。注意输入每一行数据时候在神经网络中会加入bias偏量，神经网络的层数和每层个数为自定义，搞了很久才知道输入矩阵多了一个维度，权重和后向传播更新的delta都是每列神经元之间的关系，关于s形函数暂时用了两种，分别是logistic() 和 tanh() 效果差不多，简单的模型作为笔记学习使用。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import numpy as npdef tanh(x): return np.tanh(x)def tanh_deriv(x): return 1.0 - np.tanh(x) * np.tanh(x)def logistic(x): return 1 / (1 + np.exp(-x))def logistic_derivative(x): return logistic(x) * (1 - logistic(x))class NeuralNetwork: def __init__(self, layers, activation='tanh'): if activation == 'logistic': self.activation = logistic self.activation_deriv = logistic_derivative elif activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_deriv self.weights = [] for i in range(1, len(layers) - 1): self.weights.append( (2 * np.random.random((layers[i - 1] + 1, layers[i] + 1)) - 1) * 0.25) self.weights.append( (2 * np.random.random((layers[-2] + 1, layers[-1])) - 1) * 0.25) def fit(self, X, y, learning_rate=0.2, epochs=10000): X = np.atleast_2d(X) temp = np.ones([X.shape[0], X.shape[1] + 1]) temp[:, 0:-1] = X # adding the bias unit to the input layer X = temp y = np.array(y) for k in range(epochs): i = np.random.randint(X.shape[0]) a = [X[i]] for l in range(len(self.weights)): # going forward network, for each layer a.append(self.activation(np.dot(a[l], self.weights[l]))) error = y[i] - a[-1] # Computer the error at the top layer # For output layer, Err calculation (delta is updated error) deltas = [error * self.activation_deriv(a[-1])] # Staring backprobagation for l in range(len(a) - 2, 0, -1): deltas.append(deltas[-1].dot(self.weights[l].T) * self.activation_deriv(a[l])) deltas.reverse() for i in range(len(self.weights)): layer = np.atleast_2d(a[i]) delta = np.atleast_2d(deltas[i]) self.weights[i] += learning_rate * layer.T.dot(delta) def predict(self, x): x = np.array(x) temp = np.ones(x.shape[0] + 1) temp[0:-1] = x a = temp for l in range(0, len(self.weights)): a = self.activation(np.dot(a, self.weights[l])) return ann = NeuralNetwork([2, 2, 3, 1], 'tanh')X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])y = np.array([0, 1, 1, 0])nn.fit(X, y)for i in [[0, 0], [0, 1], [1, 0], [1, 1]]: print(i, nn.predict(i))","categories":[],"tags":[{"name":"Machne Learning","slug":"Machne-Learning","permalink":"http://sunlanchang.github.io/tags/Machne-Learning/"}]}]}