{"meta":{"title":"Jason's Blog","subtitle":null,"description":"自律给我自由","author":"Jason Sun","url":"http://sunlanchang.github.io"},"pages":[{"title":"关于","date":"2017-10-20T16:47:49.000Z","updated":"2017-10-22T11:33:44.605Z","comments":true,"path":"about/index.html","permalink":"http://sunlanchang.github.io/about/index.html","excerpt":"","text":"尝试使用hexo来写博客，看看有什么惊喜的地方。"},{"title":"categories","date":"2017-10-22T14:44:02.000Z","updated":"2017-10-22T14:44:28.603Z","comments":true,"path":"categories/index.html","permalink":"http://sunlanchang.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-10-22T14:43:24.000Z","updated":"2017-10-22T14:43:53.880Z","comments":true,"path":"tags/index.html","permalink":"http://sunlanchang.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Python写一个简单的神经网络","slug":"测试文档","date":"2017-10-20T16:43:50.000Z","updated":"2017-10-22T11:31:44.440Z","comments":true,"path":"2017/10/21/测试文档/","link":"","permalink":"http://sunlanchang.github.io/2017/10/21/测试文档/","excerpt":"","text":"简单的神经网络算法，包括基本的后向传播BP算法，前向传播算法，更新权重使用的梯度下降算法，基本的框架算是有了，学习使用。注意输入每一行数据时候在神经网络中会加入bias偏量，神经网络的层数和每层个数为自定义，搞了很久才知道输入矩阵多了一个维度，权重和后向传播更新的delta都是每列神经元之间的关系，关于s形函数暂时用了两种，分别是logistic() 和 tanh() 效果差不多，简单的模型作为笔记学习使用。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import numpy as npdef tanh(x): return np.tanh(x)def tanh_deriv(x): return 1.0 - np.tanh(x) * np.tanh(x)def logistic(x): return 1 / (1 + np.exp(-x))def logistic_derivative(x): return logistic(x) * (1 - logistic(x))class NeuralNetwork: def __init__(self, layers, activation='tanh'): if activation == 'logistic': self.activation = logistic self.activation_deriv = logistic_derivative elif activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_deriv self.weights = [] for i in range(1, len(layers) - 1): self.weights.append( (2 * np.random.random((layers[i - 1] + 1, layers[i] + 1)) - 1) * 0.25) self.weights.append( (2 * np.random.random((layers[-2] + 1, layers[-1])) - 1) * 0.25) def fit(self, X, y, learning_rate=0.2, epochs=10000): X = np.atleast_2d(X) temp = np.ones([X.shape[0], X.shape[1] + 1]) temp[:, 0:-1] = X # adding the bias unit to the input layer X = temp y = np.array(y) for k in range(epochs): i = np.random.randint(X.shape[0]) a = [X[i]] for l in range(len(self.weights)): # going forward network, for each layer a.append(self.activation(np.dot(a[l], self.weights[l]))) error = y[i] - a[-1] # Computer the error at the top layer # For output layer, Err calculation (delta is updated error) deltas = [error * self.activation_deriv(a[-1])] # Staring backprobagation for l in range(len(a) - 2, 0, -1): deltas.append(deltas[-1].dot(self.weights[l].T) * self.activation_deriv(a[l])) deltas.reverse() for i in range(len(self.weights)): layer = np.atleast_2d(a[i]) delta = np.atleast_2d(deltas[i]) self.weights[i] += learning_rate * layer.T.dot(delta) def predict(self, x): x = np.array(x) temp = np.ones(x.shape[0] + 1) temp[0:-1] = x a = temp for l in range(0, len(self.weights)): a = self.activation(np.dot(a, self.weights[l])) return ann = NeuralNetwork([2, 2, 3, 1], 'tanh')X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])y = np.array([0, 1, 1, 0])nn.fit(X, y)for i in [[0, 0], [0, 1], [1, 0], [1, 1]]: print(i, nn.predict(i))","categories":[],"tags":[{"name":"Machne Learning","slug":"Machne-Learning","permalink":"http://sunlanchang.github.io/tags/Machne-Learning/"}]}]}