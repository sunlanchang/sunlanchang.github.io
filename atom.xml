<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jason&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunlanchang.github.io/"/>
  <updated>2017-10-22T11:31:44.440Z</updated>
  <id>http://sunlanchang.github.io/</id>
  
  <author>
    <name>Jason Sun</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python写一个简单的神经网络</title>
    <link href="http://sunlanchang.github.io/2017/10/21/%E6%B5%8B%E8%AF%95%E6%96%87%E6%A1%A3/"/>
    <id>http://sunlanchang.github.io/2017/10/21/测试文档/</id>
    <published>2017-10-20T16:43:50.000Z</published>
    <updated>2017-10-22T11:31:44.440Z</updated>
    
    <content type="html"><![CDATA[<p>简单的神经网络算法，包括基本的后向传播<code>BP</code>算法，前向传播算法，更新权重使用的梯度下降算法，基本的框架算是有了，学习使用。<br>注意输入每一行数据时候在神经网络中会加入<code>bias</code>偏量，神经网络的层数和每层个数为自定义，搞了很久才知道输入矩阵多了一个维度，权重和后向传播更新的delta都是每列神经元之间的关系，关于s形函数暂时用了两种，分别是<code>logistic()</code> 和 <code>tanh()</code> 效果差不多，简单的模型作为笔记学习使用。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.tanh(x)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh_deriv</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1.0</span> - np.tanh(x) * np.tanh(x)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_derivative</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> logistic(x) * (<span class="number">1</span> - logistic(x))</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">NeuralNetwork</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layers, activation=<span class="string">'tanh'</span>)</span>:</span></div><div class="line">        <span class="keyword">if</span> activation == <span class="string">'logistic'</span>:</div><div class="line">            self.activation = logistic</div><div class="line">            self.activation_deriv = logistic_derivative</div><div class="line">        <span class="keyword">elif</span> activation == <span class="string">'tanh'</span>:</div><div class="line">            self.activation = tanh</div><div class="line">            self.activation_deriv = tanh_deriv</div><div class="line"></div><div class="line">        self.weights = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(layers) - <span class="number">1</span>):</div><div class="line">            self.weights.append(</div><div class="line">                (<span class="number">2</span> * np.random.random((layers[i - <span class="number">1</span>] + <span class="number">1</span>, layers[i] + <span class="number">1</span>)) - <span class="number">1</span>) * <span class="number">0.25</span>)</div><div class="line">        self.weights.append(</div><div class="line">            (<span class="number">2</span> * np.random.random((layers[<span class="number">-2</span>] + <span class="number">1</span>, layers[<span class="number">-1</span>])) - <span class="number">1</span>) * <span class="number">0.25</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y, learning_rate=<span class="number">0.2</span>, epochs=<span class="number">10000</span>)</span>:</span></div><div class="line">        X = np.atleast_2d(X)</div><div class="line">        temp = np.ones([X.shape[<span class="number">0</span>], X.shape[<span class="number">1</span>] + <span class="number">1</span>])</div><div class="line">        temp[:, <span class="number">0</span>:<span class="number">-1</span>] = X  <span class="comment"># adding the bias unit to the input layer</span></div><div class="line">        X = temp</div><div class="line">        y = np.array(y)</div><div class="line"></div><div class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(epochs):</div><div class="line">            i = np.random.randint(X.shape[<span class="number">0</span>])</div><div class="line">            a = [X[i]]</div><div class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> range(len(self.weights)):  <span class="comment"># going forward network, for each layer</span></div><div class="line">                a.append(self.activation(np.dot(a[l], self.weights[l])))</div><div class="line">            error = y[i] - a[<span class="number">-1</span>]  <span class="comment"># Computer the error at the top layer</span></div><div class="line">            <span class="comment"># For output layer, Err calculation (delta is updated error)</span></div><div class="line">            deltas = [error * self.activation_deriv(a[<span class="number">-1</span>])]</div><div class="line">            <span class="comment"># Staring backprobagation</span></div><div class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> range(len(a) - <span class="number">2</span>, <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">                deltas.append(deltas[<span class="number">-1</span>].dot(self.weights[l].T)</div><div class="line">                              * self.activation_deriv(a[l]))</div><div class="line">            deltas.reverse()</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.weights)):</div><div class="line">                layer = np.atleast_2d(a[i])</div><div class="line">                delta = np.atleast_2d(deltas[i])</div><div class="line">                self.weights[i] += learning_rate * layer.T.dot(delta)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = np.array(x)</div><div class="line">        temp = np.ones(x.shape[<span class="number">0</span>] + <span class="number">1</span>)</div><div class="line">        temp[<span class="number">0</span>:<span class="number">-1</span>] = x</div><div class="line">        a = temp</div><div class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">0</span>, len(self.weights)):</div><div class="line">            a = self.activation(np.dot(a, self.weights[l]))</div><div class="line">        <span class="keyword">return</span> a</div><div class="line"></div><div class="line"></div><div class="line">nn = NeuralNetwork([<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>], <span class="string">'tanh'</span>)</div><div class="line">X = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</div><div class="line">y = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</div><div class="line">nn.fit(X, y)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]:</div><div class="line">    print(i, nn.predict(i))</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;简单的神经网络算法，包括基本的后向传播&lt;code&gt;BP&lt;/code&gt;算法，前向传播算法，更新权重使用的梯度下降算法，基本的框架算是有了，学习使用。&lt;br&gt;注意输入每一行数据时候在神经网络中会加入&lt;code&gt;bias&lt;/code&gt;偏量，神经网络的层数和每层个数为自定义，搞了很
      
    
    </summary>
    
    
      <category term="Machne Learning" scheme="http://sunlanchang.github.io/tags/Machne-Learning/"/>
    
  </entry>
  
</feed>
